---
title: "Swift UVOT Workbook"
author: "Adam Gonzalez"
output: 
  github_document:
---

This workbook guides you through my analysis workflow for multi-wavelength Swift
UVOT observing campaigns, specifically for the 2022 campaign on NGC 6814 as
published in [my paper](https://academic.oup.com/mnras/article/527/3/5569/7425644),
though the workflow and methods can be applied to any campaign.

First, we need to load the method functions as well as the ggplot2 package with
my custom theme. The functions.R file contains all of the code for the methods
used throughout this workbook, so you should go over them to actually understand
what is happening at each step.

```{r loadpackages}
source("functions.R")
library(ggplot2)
paper.theme <- theme_bw() +
  theme(panel.border = element_rect(fill = NA, linewidth = 1),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.ticks = element_line(colour = "black", linewidth = 0.5),
        axis.ticks.length = unit(-0.25, "cm"),
        axis.text.x = element_text(colour = "black", size = 12, margin = (unit(c(t = 0.4, r = 0, b = 0.1, l = 0), "cm"))),
        axis.text.y = element_text(colour = "black", size = 12, margin = (unit(c(t = 0, r = 0.4, b = 0, l = 0.1), "cm")), angle = 90, hjust = 0.5),
        text = element_text(colour = "black", size = 14),
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
theme_set(paper.theme)
```

Our first task is to process the Swift UVOT data, which is not performed in this
workbook. Swift UVOT data can be publicly accessed via the [HEASARC data archive](https://heasarc.gsfc.nasa.gov/docs/archive.html). Once you've downloaded
the data, you'll need to actually process it using the tasks included in [HEASoft](https://heasarc.gsfc.nasa.gov/docs/software/lheasoft/) with the latest
[calibration files](https://heasarc.gsfc.nasa.gov/docs/heasarc/caldb/caldb_intro.html).
You'll need to run $\texttt{uvotsource}$ on the UVOT images to get the outputs 
expected as inputs for the methods used here. Once you've done all of that, you 
can begin the analysis process.

I've placed my $\texttt{uvotsource}$ output FITS files in the uvotsource_output/
directory. We'll start by loading all of the output files and merging them into
a single R data frame. The data frame will contain information on the observation
ID number, start time, UVOT filter, and flux values in units specified by the user,
which in this case will be milliJanskys. 

```{r readUVOTFITS}
uvot.lc <- lc.uvot("uvotsource_outputfiles", "mJy")
```

With all of the information merged into a single data frame, we can now focus on
the 2022 epoch. We can use the HEASARC [xTime](https://heasarc.gsfc.nasa.gov/cgi-bin/Tools/xTime/xTime.pl)
tool to convert the 2022 epoch start date of 28 Aug 2022 into units of Swift 
Mission Elapsed Time (MET), which yields MET = 683337629.755 seconds. Then, we 
can break the merged data frame into separate data frames per UVOT filter.

```{r prep2022data}
# Filter for only the 2022 observing epoch
met.aug282022 <- 683337629.755
day.seconds <- 86164.0905
uvot.lc$TIME <- (uvot.lc$TIME - met.aug282022)/day.seconds
uvot.lc$TIME.ERR <- uvot.lc$TIME.ERR/day.seconds
uvot.lc <- uvot.lc[which(uvot.lc$TIME >= 0),]

# Break the master light curve into separate filters
w2.lc <- uvot.lc[which(uvot.lc$FILTER == "w2"), c("TIME", "TIME.ERR", "RATE", "RATE.ERR")]
m2.lc <- uvot.lc[which(uvot.lc$FILTER == "m2"), c("TIME", "TIME.ERR", "RATE", "RATE.ERR")]
w1.lc <- uvot.lc[which(uvot.lc$FILTER == "w1"), c("TIME", "TIME.ERR", "RATE", "RATE.ERR")]
u.lc <- uvot.lc[which(uvot.lc$FILTER == "uu"), c("TIME", "TIME.ERR", "RATE", "RATE.ERR")]
b.lc <- uvot.lc[which(uvot.lc$FILTER == "bb"), c("TIME", "TIME.ERR", "RATE", "RATE.ERR")]
v.lc <- uvot.lc[which(uvot.lc$FILTER == "vv"), c("TIME", "TIME.ERR", "RATE", "RATE.ERR")]
```

Now, let's have a look at the data for one of our filters. We'll take the W2
filter as our base example moving forward as it will be our so-called reference
band later on.

```{r plotW2lightcurve}
# Define the colour palette
colour.palette <- palette.colors(n = 6, palette = "Dark2")
w2.colour <- colour.palette[4]
m2.colour <- colour.palette[3]
w1.colour <- colour.palette[1]
u.colour <- colour.palette[5]
b.colour <- colour.palette[6]
v.colour <- colour.palette[2]
plot.colours <- c(w2.colour, m2.colour, w1.colour, u.colour, b.colour, v.colour)

# Plot the W2 band light curve
ggplot(data = w2.lc) +
  geom_errorbarh(mapping = aes(xmin = TIME-TIME.ERR, xmax = TIME+TIME.ERR, y = RATE), height = 0, colour = w2.colour) +
  geom_errorbar(mapping = aes(x = TIME, ymin = RATE-RATE.ERR, ymax = RATE+RATE.ERR), width = 0, colour = w2.colour) +
  geom_point(mapping = aes(x = TIME, y = RATE), colour = w2.colour) +
  xlab("Days since 28 Aug 2022") +
  ylab("Flux [mJy]") +
  guides(colour = "none")
```

As we can see, the W2 light curve exhibits significant variability on both short
and long time-scales. Notice, however, that there are some instances where the 
source flux exhibits rapid and significant dips. These so-called 'drop-outs' are 
associated with malfunctioning pixels on the UVOT detector, and are thus not due
to intrinsic source variability. We'll need to clean those up, and to do that 
we'll follow the method in the appendix of [Edelson et al. (2015)](10.1088/0004-637X/806/1/129).

```{r dropoutremoval}
w2.lc.clean <- remove.dropouts(w2.lc, minimum.separation = 1.5)
m2.lc.clean <- remove.dropouts(m2.lc, minimum.separation = 1.5)
w1.lc.clean <- remove.dropouts(w1.lc, minimum.separation = 1.5)
u.lc.clean <- remove.dropouts(u.lc, minimum.separation = 1.5)
b.lc.clean <- remove.dropouts(b.lc, minimum.separation = 1.5)
v.lc.clean <- remove.dropouts(v.lc, minimum.separation = 1.5)
```

The minimum.separation = 1.5 parameter indicates that we want to remove drop-outs
with a minimum separation of at least 1.5 days between them. Let's have a look at
how the cleaned data compare to the unfiltered data.

```{r dropoutcomparison}
ggplot() +
  geom_errorbarh(data = w2.lc, mapping = aes(xmin = TIME-TIME.ERR, xmax = TIME+TIME.ERR, y = RATE), height = 0, colour = 'grey') +
  geom_errorbar(data = w2.lc, mapping = aes(x = TIME, ymin = RATE-RATE.ERR, ymax = RATE+RATE.ERR), width = 0, colour = 'grey') +
  geom_point(data = w2.lc, mapping = aes(x = TIME, y = RATE), colour = 'grey') +
  
  geom_errorbarh(data = w2.lc.clean, mapping = aes(xmin = TIME-TIME.ERR, xmax = TIME+TIME.ERR, y = RATE), height = 0, colour = w2.colour) +
  geom_errorbar(data = w2.lc.clean, mapping = aes(x = TIME, ymin = RATE-RATE.ERR, ymax = RATE+RATE.ERR), width = 0, colour = w2.colour) +
  geom_point(data = w2.lc.clean, mapping = aes(x = TIME, y = RATE), colour = w2.colour) +
  
  xlab("Days since 28 Aug 2022") +
  ylab("Flux [mJy]") +
  guides(colour = "none")
```

As we can see, only 2 exposures were removed in the W2 light curve, the second
of which very clearly looks suspicious compared to the rest of the light curve.

The simplest quantification of the variability process is fractional variability, 
which offers a straightforward way in which we can assess the degree of variability
in a light curve. [Edelson et al. (2002)](10.1086/323779) and [Vaughan et al. (2003)](10.1046/j.1365-2966.2003.07042.x)
both present formalisms for computing fractional variability (Fvar), with the 
latter method producing error bars ~0.5x that of the former.

We can compute Fvar in each UVOT filter, and then plot the results as a function
of filter centroid wavelength according to [Poole et al. (2008)](10.1111/j.1365-2966.2007.12563.x). 

```{r fvar}
w2.fvar <- calc.fvar(w2.lc.clean)
m2.fvar <- calc.fvar(m2.lc.clean)
w1.fvar <- calc.fvar(w1.lc.clean)
u.fvar <- calc.fvar(u.lc.clean)
b.fvar <- calc.fvar(b.lc.clean)
v.fvar <- calc.fvar(v.lc.clean)

fvar.spectrum <- data.frame(WAVELENGTH = c(1928, 2246, 2600, 3465, 4392, 5468),
                            FVAR = c(w2.fvar$FVAR, m2.fvar$FVAR, w1.fvar$FVAR, u.fvar$FVAR, b.fvar$FVAR, v.fvar$FVAR),
                            FVAR.ERR = c(w2.fvar$FVAR.ERR, m2.fvar$FVAR.ERR, w1.fvar$FVAR.ERR, u.fvar$FVAR.ERR, b.fvar$FVAR.ERR, v.fvar$FVAR.ERR))

ggplot(data = fvar.spectrum) +
  geom_errorbar(mapping = aes(x = WAVELENGTH, ymin = FVAR-FVAR.ERR, ymax = FVAR+FVAR.ERR, colour = as.factor(WAVELENGTH)), width = 0) +
  geom_point(mapping = aes(x = WAVELENGTH, y = FVAR, colour = as.factor(WAVELENGTH))) +
  scale_colour_manual(values = plot.colours) +
  xlab("Observed wavelength [Angstrom]") + scale_x_log10() +
  ylab("Fractional variability") +
  guides(colour = "none")
```

We can already see some interesting behaviour! Fvar seems to decrease log-linearly
with wavelength. This suggests some damping mechanism with increasing strength at
longer wavelengths. A variable accretion disc emission component alongside an 
invariant host-galaxy emission component can broadly reproduce such behaviour.

To more thoroughly characterize the variability process in each waveband, we can
compute the structure function (see [Collier & Peterson 2001](10.1086/321517) 
and [Gallo et al. 2018](10.1093/mnras/sty1134)). 

```{r structurefunction}
w2.sf <- calc.sf(w2.lc.clean)
m2.sf <- calc.sf(m2.lc.clean)
w1.sf <- calc.sf(w1.lc.clean)
u.sf <- calc.sf(u.lc.clean)
b.sf <- calc.sf(b.lc.clean)
v.sf <- calc.sf(v.lc.clean)

ggplot(data = w2.sf[-1,]) +
  geom_errorbar(mapping = aes(x = tau, ymin = val-err, ymax = val+err), width = 0, colour = w2.colour) +
  geom_point(mapping = aes(x = tau, y = val), colour = w2.colour) +
  xlab(bquote(tau~"[days]")) + scale_x_log10() +
  ylab(bquote(S(tau))) + scale_y_log10() +
  guides(colour = "none")
```

(Insert a section about structure function fitting here!)

Next, we can compute the time-lag between two wavebands. Typically, one filter
is selected as the reference band to which all other filters are compared. Often,
the W2 band is selected as the reference band as it is least contaminated by 
emission components outside of the AGN central engine. In some cases, however,
longer wavelength bands should be used, such as those sources that are extremely
reddened due to dust either in the Milky Way or host galaxy. In our case, we'll
use the W2 filter as our reference band. We'll use the interpolated cross-
correlation function (ICCF; [Gaskell & Sparke 1986](10.1086/164238), [Gaskell & Peterson 1987](10.1086/191216), [White & Peterson 1994](10.1086/133456), [Edelson et al. 2019](10.3847/1538-4357/aaf3b4)) 
to evaluate the time-lags in each filter with respect to W2. Moreover, we'll 
limit the lag calculation to those time-scales <1/3 the light curve length, as 
signals with <3 cycles are ambiguously detected. We'll use a time sampling of 0.1
days.

```{r crosscorrelation}
iccf.w2.m2 <- calc.iccf(w2.lc.clean, m2.lc.clean, delta.tau = 0.1, 
                        max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                        max(m2.lc.clean$TIME)-min(m2.lc.clean$TIME)))/3
                        )
iccf.w2.w1 <- calc.iccf(w2.lc.clean, w1.lc.clean, delta.tau = 0.1, 
                        max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                        max(w1.lc.clean$TIME)-min(w1.lc.clean$TIME)))/3
                        )
iccf.w2.u <- calc.iccf(w2.lc.clean, u.lc.clean, delta.tau = 0.1, 
                       max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                       max(u.lc.clean$TIME)-min(u.lc.clean$TIME)))/3
                       )
iccf.w2.b <- calc.iccf(w2.lc.clean, b.lc.clean, delta.tau = 0.1, 
                       max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                       max(b.lc.clean$TIME)-min(b.lc.clean$TIME)))/3
                       )
iccf.w2.v <- calc.iccf(w2.lc.clean, v.lc.clean, delta.tau = 0.1, 
                       max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                       max(v.lc.clean$TIME)-min(v.lc.clean$TIME)))/3
                       )

ggplot() + 
  geom_vline(xintercept = 0, colour = "grey") + geom_hline(yintercept = 0, colour = "grey") +
  geom_path(data = iccf.w2.m2, mapping = aes(x = tau, y = iccf), colour = m2.colour) +
  geom_path(data = iccf.w2.w1, mapping = aes(x = tau, y = iccf), colour = w1.colour) +
  geom_path(data = iccf.w2.u, mapping = aes(x = tau, y = iccf), colour = u.colour) +
  geom_path(data = iccf.w2.b, mapping = aes(x = tau, y = iccf), colour = b.colour) +
  geom_path(data = iccf.w2.v, mapping = aes(x = tau, y = iccf), colour = v.colour) +
  xlab(bquote(tau~"[days]")) +
  ylab(bquote(ICCF(tau))) +
  guides(colour = "none")
```

At this point, it is clear that when compared to the W2 filter all other UVOT
filters are highly correlated with nearly 0 time-lag. However, there are some 
questions that we should ask ourselves: 1) how do we know what we're seeing is
real and not just noise? and 2) how do we measure the time-lag?

In the literature, question 1 is seldom addressed, but it's actually quite an 
important point that will be directly related to our answer for question 2 later. 
The basis of our answer is to try to compute what the ICCF should look like if 
the variability was just 'random' noise. In practice, AGN variability follows a
'red' noise process, with increasing variability at longer wavelengths. Thus, we
need to simulate a sample of realistic AGN red noise light curves to compute the
expected 'noise' level in the ICCF. To do this, we'll use the simple method of
[Timmer & Konig (1995)](https://ui.adsabs.harvard.edu/abs/1995A%26A...300..707T/abstract)
in which an AGN light curve is simulated based on an input power spectral density
(PSD). The structure function slope (alpha) above is related to the PSD slope 
(beta) as beta = 1 + alpha. Therefore, considering our W2 reference band gave us
beta = 0.5 we can use alpha = 1.5 and simulate N = 1000 W2 light curves that we 
can compute the ICCF for against our observed W2 light curve. Since this process
can take a while (depending on the number of simulations that you want to do),
we're going to parallelize it over 10 CPU cores (adjust down if you don't have
that many!) using the R packages foreach and doParallel. 

```{r crosscorrelationsimulations}
### FIX THIS!!!!vvvvvvv

### set the light curves
compare <- lc.w1 ; comparecolour <- plot.colours[4] ; compare.orig <- compare
base <- read.table(paste(epoch, "/", base.results$filename[band.num], ".dat", sep = ""), header = T) ; basecolour <- plot.colours[7]
t.diff <- min(c(compare$TIME, base$TIME))
compare$TIME <- compare$TIME-t.diff
base$TIME <- base$TIME-t.diff
dtau <- 0.1
max.lag.time <- round(max(c(compare$TIME, base$TIME))/2)*2/2


### COMPUTE THE ICCF ###########################################################
my.iccf <- calc.myiccf(base, compare, dtau, max.lag.time)





### PLOT ICCF + CONTOURS + INITIAL GAUSSIAN FIT TO CENTROID ####################
max.iccf.x <- round(max(my.iccf$tau)/5)*5
peak.tau <- 5
peak.idxs <- which(my.iccf$iccf > my.iccf$p99 & abs(my.iccf$tau) <= peak.tau)


### MEASURE THE CENTROIDS ON MULTI-CORE ########################################
n.iter <- num
cores = 10
cl <- makeCluster(cores[1])
registerDoParallel(cl)
# start.time <- Sys.time()
centroids.parallel <- foreach(i=1:n.iter, .combine=rbind, .packages="heatools") %dopar% {
  base.tmp <- base
  compare.tmp <- compare

  ### perform flux randomisation using error bars as st.dev. with mean = 0
  base.tmp$RATE <- base.tmp$RATE + rnorm(n = length(base.tmp$RATE.ERR), mean = 0, sd = base.tmp$RATE.ERR)
  compare.tmp$RATE <- compare.tmp$RATE + rnorm(n = length(compare.tmp$RATE.ERR), mean = 0, sd = compare.tmp$RATE.ERR)

  ### resample the light curves keeping only unique times
  base.rand.idx <- floor(runif(length(base.tmp$TIME), min=1, max=length(base.tmp$TIME)+1))
  compare.rand.idx <- floor(runif(length(compare.tmp$TIME), min=1, max=length(compare.tmp$TIME)+1))
  base.tmp <- base.tmp[unique(sort(base.rand.idx)),]
  compare.tmp <- compare.tmp[unique(sort(compare.rand.idx)),]

  ### compute the new iccf
  tmp.iccf <- calc.myiccf(base.tmp, compare.tmp, dtau, max.lag = max.lag.time)
  tmp.iccf <- tmp.iccf[which(is.nan(my.iccf$iccf) == F),]
  # tmp.iccf$iccf

  ### extract tmp.iccf time bins where it is > 99% contour of REAL iccf, then fit
  tmp.peak.idxs <- which(tmp.iccf$iccf > my.iccf$p99 & abs(tmp.iccf$tau) <= peak.tau)
  tmp.iccf.peak <- tmp.iccf[tmp.peak.idxs,]
  tmp.iccf.peak <- tmp.iccf.peak[which(is.na(tmp.iccf.peak$iccf) == F),]
  # tmp.gaussian.fit <- optim(par = c(start.norm, start.mean, start.width, start.const),
  #                           fn = minimize.chisq,
  #                           method = "L-BFGS-B",
  #                           lower = c(1e-1, -5, 0.1, -1),
  #                           upper = c(1e1, 5, 5, 1),
  #                           control = list(maxit = 1000),
  #                           # control = list(fnscale = 1e6, maxit = 1000),
  #                           data = data.frame(x = tmp.iccf.peak$tau, y = tmp.iccf.peak$iccf, y.err = tmp.iccf.peak$iccf.err))
  # centroids <- tmp.gaussian.fit$par[2]
  # centroids <- length(tmp.peak.idxs)
  centroids <- weighted.mean(tmp.iccf.peak$tau, w = tmp.iccf.peak$iccf)

  centroids
}
stopCluster(cl)
# end.time <- Sys.time()
# end.time - start.time

# for (i in 1:10){
#   peak.idxs <- which(centroids.parallel[,i] > my.iccf$p99 & abs(my.iccf$tau) <= peak.tau)
#   print(
#   ggplot() +
#     geom_vline(xintercept = 0.0, lty = 5, lwd = 0.25) + geom_hline(yintercept = c(-1, 0, 1), lty = c(3, 5, 3), lwd = 0.25) +
#
#     geom_step(aes(x = my.iccf$tau-dtau/2, y = centroids.parallel[,i]), colour = "grey", lwd = 0.5) +
#     geom_errorbarh(aes(xmin = my.iccf$tau-dtau/2, xmax = my.iccf$tau+dtau/2, y = centroids.parallel[,i]), colour = "grey", height = 0, lwd = 0.5) +
#     geom_point(data = my.iccf[peak.idxs,], aes(x = tau, y = centroids.parallel[peak.idxs,i]), colour = "black", shape = 19, size = 2) +
#     geom_path(aes(x = my.iccf$tau, y = my.iccf$p99), colour = "firebrick1", lwd = 1) +
#
#     # coord_cartesian(ylim = c(0, 1), xlim = c(-10, 10)) +
#     # coord_cartesian(ylim = c(0, 1), xlim = c(-max.iccf.x, max.iccf.x)) +
#     xlab(bquote(italic(t)[W1]-italic(t)[X]/days)) + ylab(bquote(ICCF)) +
#     scale_x_continuous(breaks = seq(from = -5e3, to = 5e3, by = 1e1), sec.axis = dup_axis(name = NULL, labels = rep(" ", length.out = length(seq(from = -5e3, to = 5e3, by = 1e1))))) +
#     scale_y_continuous(breaks = seq(from = -2, to = 2, by = 0.5), sec.axis = dup_axis(name = NULL, labels = rep(" ", length.out = length(seq(from = -2, to = 2, by = 0.5))))) +
#     guides(colour = "none")
#   )
# }

centroids <- unname(centroids.parallel[,1]) ; rm(centroids.parallel)
length(centroids[which(is.nan(centroids) == F)])
centroids <- centroids[which(is.nan(centroids) == F)]

dt <- 0.25
centroid.histogram <- hist(centroids, breaks = seq(from = -max.lag.time, to = max.lag.time, by = dt), plot = F)
lag.shaded <- data.frame(x = seq(from = unname(quantile(centroids, 0.16)), to = unname(quantile(centroids, 0.84)), by = 0.01))


### PLOT FINAL ICCF + CONTOUR + CENTROID DISTRIBUTION ##########################
# png(paste(epoch, "/iccfs/myICCFsgfilt_N", num, "_W1base_", base.results$band[band.num], "comp_Rplot.png", sep = ""), width = 6, height = 4, units = "in", res = 300)
png(paste(epoch, "/iccfs/myICCFunfilt_W1base_", base.results$band[band.num], "comp_Rplot.png", sep = ""), width = 6, height = 4, units = "in", res = 300)
print(
ggplot() +
  geom_vline(xintercept = 0.0, lty = 5, lwd = 0.25) + geom_hline(yintercept = c(-1, 0, 1), lty = c(3, 5, 3), lwd = 0.25) +
  geom_col(aes(x = centroid.histogram$mids, y = centroid.histogram$counts/max(centroid.histogram$counts)), width = dt, fill = basecolour, alpha = 0.5) +
  geom_step(data = my.iccf, aes(x = tau-dtau/2, y = iccf), colour = "black", lwd = 0.5) +
  # geom_step(data = my.iccf, aes(x = tau-dtau/2, y = p99), colour = "firebrick1", lwd = 1) +
  geom_path(data = my.iccf, aes(x = tau, y = p99), colour = "firebrick1", lwd = 1) +
  coord_cartesian(ylim = c(0, 1), xlim = c(-10, 10)) +
  # coord_cartesian(ylim = c(0, 1), xlim = c(-max.iccf.x, max.iccf.x)) +
  xlab(bquote(italic(t)[W1]-italic(t)[X]/days)) + ylab(bquote(ICCF)) +
  scale_x_continuous(breaks = seq(from = -100, to = 100, by = 5), sec.axis = dup_axis(name = NULL, labels = rep(" ", length.out = length(seq(from = -100, to = 100, by = 5))))) +
  scale_y_continuous(breaks = seq(from = -2, to = 2, by = 0.25), sec.axis = dup_axis(name = NULL, labels = rep(" ", length.out = length(seq(from = -2, to = 2, by = 0.25))))) +
  guides(colour = "none")
)
dev.off()

### output the 68% confidence region of centroid measurement
cat(paste(round(quantile(centroids, 0.16), digits=4),
          round(quantile(centroids, 0.50), digits=4),
          round(quantile(centroids, 0.84), digits=4), sep = " , "), "\n")

base.results$lag.p16[band.num] <- as.numeric(unname(quantile(centroids, 0.16)))
base.results$lag.p50[band.num] <- as.numeric(unname(quantile(centroids, 0.50)))
base.results$lag.p84[band.num] <- as.numeric(unname(quantile(centroids, 0.84)))
base.results$num.centroids[band.num] <- length(centroids)


my.iccf <- my.iccf[which(is.na(my.iccf$iccf)==F),]
write.table(my.iccf, file = paste(epoch, "/iccfs/myICCFunfilt_W1base_", base.results$band[band.num], "comp.dat", sep = ""), quote = F, row.names = F)
write.table(centroids, file = paste(epoch, "/iccfs/myICCFunfilt_W1base_", base.results$band[band.num], "comp_centroids.dat", sep = ""), quote = F, row.names = F)
```

Now that we know that we are observing truly significant time-lag signatures, we
need to quantify the lags. We will use the flux randomization random subset 
sampling (FR/RSS) method of [Peterson et al. (1998)](10.1086/316177). This method
takes into account the different sampling of the variability process across the 
different wavebands. 

```{r crosscorrelationFRRSS}
```

However, it is well-known
that long-term trends in UV/optical AGN light curves can dominated such methods.
Reverberation off on the accretion disc, however, is expected to occur on time-
scales of ~days for systems such as NGC 6814 with black hole masses of ~1e7 Msol.
Thus, we need to remove the long-term trend in the data and re-evaluate.

We can de-trend the data using a Savitzky-Golay filter, which applies a polynomial
of a user-specified order to a moving window containing a user-specified number 
of points. But how do we choose the values? Let's start simple, with a 1st order
polynomial. Then, we just need to determine how many points wide our moving
window should be. 

In the literature, the selection of smoothing window width has been somewhat 
arbitrary. Here, we'll try to do it a bit more quantitatively by using the auto 
correlation function (ACF) of the W2 filter. Since the ACF should be centered at
tau = 0 days and should evaluate to ICCF(0) = 1, let's see for what window width
we can optimize these values, i.e. let's see for what window width we can make 
the ACF as narrow as possible while still evaluating to ICCF(0) = 1.

```{r w2ACFoptimization}
w2.acf <- calc.iccf(w2.lc.clean, 
                    w2.lc.clean, 
                    delta.tau = 0.1,
                    max.lag = (max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME))/3
                    )

peak.width <- NA_real_
for (i in 2:length(w2.acf$tau)){
  if (w2.acf$iccf[i] > 0 & w2.acf$iccf[i-1] < 0 | w2.acf$iccf[i] < 0 & w2.acf$iccf[i-1] > 0){
    peak.width <- c(peak.width, i)
  }
}
peak.width <- min(abs(w2.acf$tau[peak.width[-1]]))


filter.idxs <- seq(from = 11, to = 247, by = 2)
best.filter <- data.frame(filter.width = NA_integer_,
                          time.width = NA_real_,
                          std.dev = NA_real_,
                          iccf.max = NA_real_,
                          peak.width = NA_real_)
for (filter.width in filter.idxs){
  w2.acf.sg.tmp <- calc.iccf(detrend.SavitzkyGolay(w2.lc.clean, filter.width = filter.width),
                             detrend.SavitzkyGolay(w2.lc.clean, filter.width = filter.width),
                             delta.tau = 0.1,
                             max.lag = (max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME))/3)
  peak.width.sg <- NA_real_
  for (i in 2:length(w2.acf.sg.tmp$tau)){
    if (w2.acf.sg.tmp$iccf[i] > 0 & w2.acf.sg.tmp$iccf[i-1] < 0 | w2.acf.sg.tmp$iccf[i] < 0 & w2.acf.sg.tmp$iccf[i-1] > 0){
      peak.width.sg <- c(peak.width.sg, i)
    }
  }
  peak.width.sg <- min(abs(w2.acf.sg.tmp$tau[peak.width.sg[-1]]))
  
  if (length(which(is.na(w2.acf.sg.tmp$iccf) == TRUE)) > 0){
    cat(paste("***NaN found in DCF for filter.width = ", filter.width, "!\n", sep = ""))
    next
  } else if (is.na(w2.acf.sg.tmp$iccf) == F){
    best.filter <- rbind(best.filter, data.frame(filter.width = filter.width,
                                                 time.width = filter.width*median(diff(w2.lc.clean$TIME)),
                                                 # std.dev =  sd(w2.acf.sg.tmp$iccf),
                                                 # iccf.max = max(w2.acf.sg.tmp$iccf)))
                                                 std.dev =  sd(w2.acf.sg.tmp$iccf[which(w2.acf.sg.tmp$tau <= -peak.width.sg | w2.acf.sg.tmp$tau >= peak.width.sg)]),
                                                 iccf.max = max(w2.acf.sg.tmp$iccf[which(w2.acf.sg.tmp$tau >= -peak.width.sg | w2.acf.sg.tmp$tau <= peak.width.sg)]),
                                                 peak.width = peak.width.sg))
  }
}
best.filter <- best.filter[-1,]

best.idx <- which.min(best.filter$std.dev)
ggplot(best.filter) +
  geom_path(aes(x = time.width, y = std.dev), colour = "firebrick1", lwd = 0.5) +
  geom_point(aes(x = best.filter$time.width[best.idx], y = best.filter$std.dev[best.idx]), colour = "dodgerblue", shape = 8, size = 3) +
  xlab(bquote(w/days)) +
  ylab(bquote(sigma[ACF])) +
  guides(colour = "none")
# best.filter[best.idx,]
```

From the plot of ACF peak width and window size above, it is clear that a width
of 83 points minimizes the ACF peak width. Considering the median time separation
between observations, this corresponds approximately to a time smoothing of ~21.6
days. Let's have a look at the trend in the W2 light curve that we've smoothed.

```{r w2smoothedlightcurve}
w2.sg.trend <- detrend.SavitzkyGolay(w2.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'SG')

ggplot() +
  geom_errorbarh(data = w2.lc.clean, mapping = aes(xmin = TIME-TIME.ERR, xmax = TIME+TIME.ERR, y = RATE), height = 0, colour = w2.colour) +
  geom_errorbar(data = w2.lc.clean, mapping = aes(x = TIME, ymin = RATE-RATE.ERR, ymax = RATE+RATE.ERR), width = 0, colour = w2.colour) +
  geom_point(data = w2.lc.clean, mapping = aes(x = TIME, y = RATE), colour = w2.colour) +

  geom_path(data = w2.sg.trend, mapping = aes(x = TIME, y = RATE), colour = "black") +
    
  xlab("Days since 28 Aug 2022") +
  ylab("Flux [mJy]") +
  guides(colour = "none")
```

Let's see what's left over when we de-trend the light curve then (original-trend).

```{r w2detrendedlightcurve}
w2.sg.detrended <- detrend.SavitzkyGolay(w2.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'detrended')

ggplot() +
  geom_hline(yintercept = 0, colour = "black") +
  geom_errorbarh(data = w2.sg.detrended, mapping = aes(xmin = TIME-TIME.ERR, xmax = TIME+TIME.ERR, y = RATE), height = 0, colour = w2.colour) +
  geom_errorbar(data = w2.sg.detrended, mapping = aes(x = TIME, ymin = RATE-RATE.ERR, ymax = RATE+RATE.ERR), width = 0, colour = w2.colour) +
  geom_point(data = w2.sg.detrended, mapping = aes(x = TIME, y = RATE), colour = w2.colour) +

  xlab("Days since 28 Aug 2022") +
  ylab("Flux [mJy]") +
  guides(colour = "none")
```

Great, the long-term trend is completely removed and what we're left with is the
rapid variability that is most likely to be associated with reverberation off of
the accretion disc. Let's see how the ACF has changed.

```{r w2detrendedACF}
w2.acf.sg <- calc.iccf(w2.sg.detrended,
                       w2.sg.detrended,
                       delta.tau = 0.1,
                       max.lag = (max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME))/3)

ggplot() +
  geom_hline(yintercept = 0, colour = "grey") + geom_vline(xintercept = 0, colour = "grey") +
  geom_path(data = w2.acf, mapping = aes(x = tau, y = iccf), colour = w2.colour) +
  geom_path(data = w2.acf.sg, mapping = aes(x = tau, y = iccf), colour = "black") +
  xlab(bquote(tau~"[days]")) +
  ylab(bquote(ICCF(tau))) +
  guides(colour = "none")
```

Armed with this optimal smoothing window width, let's apply it to all of the light
curves and re-evaluate the time-lags.

```{r detrendedcrosscorrelation}

m2.sg.detrended <- detrend.SavitzkyGolay(m2.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'detrended')
w1.sg.detrended <- detrend.SavitzkyGolay(w1.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'detrended')
u.sg.detrended <- detrend.SavitzkyGolay(u.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'detrended')
b.sg.detrended <- detrend.SavitzkyGolay(b.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'detrended')
v.sg.detrended <- detrend.SavitzkyGolay(v.lc.clean, filter.width = best.filter$filter.width[best.idx], output = 'detrended')


iccf.w2.m2.detrended <- calc.iccf(w2.sg.detrended, m2.sg.detrended, delta.tau = 0.1,
                                  max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                                  max(m2.lc.clean$TIME)-min(m2.lc.clean$TIME)))/3
                                  )
iccf.w2.w1.detrended <- calc.iccf(w2.sg.detrended, w1.sg.detrended, delta.tau = 0.1, 
                                  max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                                  max(w1.lc.clean$TIME)-min(w1.lc.clean$TIME)))/3
                                  )
iccf.w2.u.detrended <- calc.iccf(w2.sg.detrended, u.sg.detrended, delta.tau = 0.1, 
                                 max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                                 max(u.lc.clean$TIME)-min(u.lc.clean$TIME)))/3
                                 )
iccf.w2.b.detrended <- calc.iccf(w2.sg.detrended, b.sg.detrended, delta.tau = 0.1, 
                                 max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                                 max(b.lc.clean$TIME)-min(b.lc.clean$TIME)))/3
                                 )
iccf.w2.v.detrended <- calc.iccf(w2.sg.detrended, v.sg.detrended, delta.tau = 0.1, 
                                 max.lag = max(c(max(w2.lc.clean$TIME)-min(w2.lc.clean$TIME),
                                                 max(v.lc.clean$TIME)-min(v.lc.clean$TIME)))/3
                                 )

ggplot() + 
  geom_vline(xintercept = 0, colour = "grey") + geom_hline(yintercept = 0, colour = "grey") +
  geom_path(data = iccf.w2.m2.detrended, mapping = aes(x = tau, y = iccf), colour = m2.colour) +
  geom_path(data = iccf.w2.w1.detrended, mapping = aes(x = tau, y = iccf), colour = w1.colour) +
  geom_path(data = iccf.w2.u.detrended, mapping = aes(x = tau, y = iccf), colour = u.colour) +
  geom_path(data = iccf.w2.b.detrended, mapping = aes(x = tau, y = iccf), colour = b.colour) +
  geom_path(data = iccf.w2.v.detrended, mapping = aes(x = tau, y = iccf), colour = v.colour) +
  xlab(bquote(tau~"[days]")) +
  ylab(bquote(ICCF(tau))) +
  guides(colour = "none")
```






